{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install PyTDC","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install pennylane","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tdc.multi_pred import DTI\ndata = DTI(name = 'DAVIS')\nsplit = data.get_split()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tdc.multi_pred import DTI\ndata = DTI(name = 'BindingDB_Kd')\nsplit = data.get_split()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tdc.multi_pred import DTI\ndata = DTI(name = 'KIBA')\nsplit = data.get_split()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tdc.multi_pred import DTI\nfrom tdc.utils import convert_to_log\nimport pandas as pd\nfrom scipy.stats import skew\n\n# Load datasets from TDC\ndatasets = {\n    'DAVIS': DTI(name='DAVIS'),\n    'KIBA': DTI(name='KIBA'),\n    'BindingDB_Kd': DTI(name='BindingDB_Kd')\n}\n\n# Create a table to store statistics\nstats = []\n\n# Process each dataset\nfor name, dataset in datasets.items():\n    data = dataset.get_data()\n    data['Y'] = convert_to_log(data['Y'])  # Log-transform binding values\n\n    mean_val = data['Y'].mean()\n    std_val = data['Y'].std()\n    skew_val = skew(data['Y'])\n\n    stats.append({\n        'Dataset': name,\n        'Mean (log)': round(mean_val, 3),\n        'Std Dev': round(std_val, 3),\n        'Skewness': round(skew_val, 3)\n    })\n\n# Convert to DataFrame\ndf_stats = pd.DataFrame(stats)\nprint(df_stats)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Import required libraries\nfrom tdc.multi_pred import DTI\nfrom tdc.chem_utils import MolConvert\nfrom tdc.utils import convert_to_log\n\n# Load BindingDB_Kd dataset\ndata = DTI(name='BindingDB_Kd')\nsplit = data.get_split()\n\n# Convert Kd values to log scale for better regression performance\nsplit['train']['Y'] = convert_to_log(split['train']['Y'])\nsplit['test']['Y'] = convert_to_log(split['test']['Y'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Import required libraries\nfrom tdc.multi_pred import DTI\nfrom tdc.chem_utils import MolConvert\nfrom tdc.utils import convert_to_log\n\n# Load BindingDB_Kd dataset\ndata = DTI(name='DAVIS')\nsplit = data.get_split()\n\n# Convert Kd values to log scale for better regression performance\nsplit['train']['Y'] = convert_to_log(split['train']['Y'])\nsplit['test']['Y'] = convert_to_log(split['test']['Y'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Import required libraries\nfrom tdc.multi_pred import DTI\nfrom tdc.chem_utils import MolConvert\nfrom tdc.utils import convert_to_log\n\n\n# Load BindingDB_Kd dataset\ndata = DTI(name='KIBA')\nsplit = data.get_split()\n\n# Convert Kd values to log scale for better regression performance\nsplit['train']['Y'] = convert_to_log(split['train']['Y'])\nsplit['test']['Y'] = convert_to_log(split['test']['Y'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tdc.multi_pred import DTI\nfrom tdc.utils import convert_to_log\nimport numpy as np\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, explained_variance_score\n\n# Load the dataset\ndata = DTI(name=\"BindingDB_Kd\")\nsplit = data.get_split()\n\n# Filter datasets for common Drug IDs\ncommon_drug_ids = list(set(split[\"train\"][\"Drug_ID\"]) & set(split[\"test\"][\"Drug_ID\"]))\nsplit[\"train\"] = split[\"train\"][split[\"train\"][\"Drug_ID\"].isin(common_drug_ids)]\nsplit[\"test\"] = split[\"test\"][split[\"test\"][\"Drug_ID\"].isin(common_drug_ids)]\n\n# Convert Kd values to log scale\nsplit[\"train\"][\"Y\"] = convert_to_log(split[\"train\"][\"Y\"])\nsplit[\"test\"][\"Y\"] = convert_to_log(split[\"test\"][\"Y\"])\n\n# Use smaller subset for testing to avoid kernel crashes\nsplit[\"train\"] = split[\"train\"].sample(5000, random_state=42)  # Reduce to 5000 samples\nsplit[\"test\"] = split[\"test\"].sample(1000, random_state=42)    # Reduce to 1000 samples\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tdc.multi_pred import DTI\nfrom tdc.utils import convert_to_log\nimport numpy as np\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, explained_variance_score\n\n# Load the dataset\ndata = DTI(name=\"DAVIS\")\nsplit = data.get_split()\n\n# Filter datasets for common Drug IDs\ncommon_drug_ids = list(set(split[\"train\"][\"Drug_ID\"]) & set(split[\"test\"][\"Drug_ID\"]))\nsplit[\"train\"] = split[\"train\"][split[\"train\"][\"Drug_ID\"].isin(common_drug_ids)]\nsplit[\"test\"] = split[\"test\"][split[\"test\"][\"Drug_ID\"].isin(common_drug_ids)]\n\n# Convert Kd values to log scale\nsplit[\"train\"][\"Y\"] = convert_to_log(split[\"train\"][\"Y\"])\nsplit[\"test\"][\"Y\"] = convert_to_log(split[\"test\"][\"Y\"])\n\n# Use smaller subset for testing to avoid kernel crashes\nsplit[\"train\"] = split[\"train\"].sample(5000, random_state=42)  # Reduce to 5000 samples\nsplit[\"test\"] = split[\"test\"].sample(1000, random_state=42)    # Reduce to 1000 samples\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tdc.multi_pred import DTI\nfrom tdc.utils import convert_to_log\nimport numpy as np\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, explained_variance_score\n\n# Load the dataset\ndata = DTI(name=\"KIBA\")\nsplit = data.get_split()\n\n# Filter datasets for common Drug IDs\ncommon_drug_ids = list(set(split[\"train\"][\"Drug_ID\"]) & set(split[\"test\"][\"Drug_ID\"]))\nsplit[\"train\"] = split[\"train\"][split[\"train\"][\"Drug_ID\"].isin(common_drug_ids)]\nsplit[\"test\"] = split[\"test\"][split[\"test\"][\"Drug_ID\"].isin(common_drug_ids)]\n\n# Convert Kd values to log scale\nsplit[\"train\"][\"Y\"] = convert_to_log(split[\"train\"][\"Y\"])\nsplit[\"test\"][\"Y\"] = convert_to_log(split[\"test\"][\"Y\"])\n\n# Use smaller subset for testing to avoid kernel crashes\nsplit[\"train\"] = split[\"train\"].sample(5000, random_state=42)  # Reduce to 5000 samples\nsplit[\"test\"] = split[\"test\"].sample(1000, random_state=42)    # Reduce to 1000 samples\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tdc.multi_pred import DTI\n\n# Define datasets\ndatasets = {\n    'DAVIS': DTI(name='DAVIS'),\n    'KIBA': DTI(name='KIBA'),\n    'BindingDB': DTI(name='BindingDB_Kd')  # For validation only\n}\n\n# Table metadata\ntable_data = []\n\nfor name, d in datasets.items():\n    data = d.get_data()\n    n_interactions = len(data)\n    n_drugs = len(data['Drug'].unique())\n    n_targets = len(data['Target'].unique())\n\n    if name in ['DAVIS', 'KIBA']:\n        train_size = int(0.8 * n_interactions)\n        test_size = n_interactions - train_size\n        split_type = 'Training & Testing'\n    else:\n        train_size = '—'\n        test_size = '—'\n        split_type = 'Validation'\n\n    binding_metric = 'Kd' if name == 'DAVIS' else (\n        'KIBA Score' if name == 'KIBA' else 'Kd, IC50, Ki'\n    )\n    interaction_type = 'Kinase–Protein' if name in ['DAVIS', 'KIBA'] else 'Small Molecule–Protein'\n\n    table_data.append({\n        'Dataset': name,\n        'Drugs': n_drugs,\n        'Targets': n_targets,\n        'Total Interactions': n_interactions,\n        'Train (80%)': train_size,\n        'Test (20%)': test_size,\n        'Interaction Type': interaction_type,\n        'Binding Metric': binding_metric,\n        'Split Type': split_type\n    })\n\n# Convert to DataFrame and display\nimport pandas as pd\ndf_split = pd.DataFrame(table_data)\nprint(df_split)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport pennylane as qml\nimport time\nimport joblib\nfrom sklearn.svm import SVR\nfrom sklearn.kernel_approximation import Nystroem\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import (\n    mean_squared_error, mean_absolute_error, r2_score,\n    roc_auc_score\n)\nfrom scipy.stats import pearsonr\nfrom tdc.multi_pred import DTI\nfrom tdc.utils import convert_to_log\n\n# Quantum setup\nn_qubits = 8\ndev = qml.device(\"lightning.qubit\", wires=n_qubits)\n\n# Quantum feature map\ndef quantum_feature_map(x):\n    for i in range(n_qubits):\n        qml.RY(x[i % len(x)], wires=i)\n        qml.CZ(wires=[i, (i + 1) % n_qubits])\n\n# Quantum kernel function\n@qml.qnode(dev)\ndef quantum_kernel(x1, x2):\n    quantum_feature_map(x1)\n    qml.adjoint(quantum_feature_map)(x2)\n    return qml.expval(qml.PauliZ(0))\n\n# Nyström Approximation for quantum kernel\ndef compute_approximate_quantum_kernel(X, num_samples=100):\n    print(\"\\n⚡ Computing Approximate Quantum Kernel with Nyström Method...\")\n    start_time = time.time()\n    nystroem = Nystroem(kernel='rbf', n_components=num_samples)\n    X_transformed = nystroem.fit_transform(X)\n    print(f\"✅ Nyström Approximation Completed in {time.time() - start_time:.2f} sec\")\n    return X_transformed\n\n# Data processing\ndef process_dataset():\n    data = DTI(name=\"DAVIS\")\n    split = data.get_split()\n    split[\"train\"][\"Y\"] = convert_to_log(split[\"train\"][\"Y\"])\n    split[\"test\"][\"Y\"] = convert_to_log(split[\"test\"][\"Y\"])\n\n    subset_size = 500\n    X_train = np.random.rand(subset_size, 128)  # Dummy data for testing\n    X_test = np.random.rand(200, 128)  # Dummy data for testing\n    y_train = split[\"train\"][\"Y\"].values[:subset_size]\n    y_test = split[\"test\"][\"Y\"].values[:200]\n\n    scaler = StandardScaler()\n    X_train = scaler.fit_transform(X_train)\n    X_test = scaler.transform(X_test)\n\n    return X_train, X_test, y_train, y_test\n\n# Training Optimized Quantum SVR\ndef train_optimized_qsvr():\n    X_train, X_test, y_train, y_test = process_dataset()\n    \n    # Compute the Nyström approximation for the training data\n    X_train_q = compute_approximate_quantum_kernel(X_train, num_samples=50)\n\n    print(\"\\n🚀 Training Optimized Quantum SVR...\")\n    qsvr = SVR(kernel=\"linear\")\n    qsvr.fit(X_train_q, y_train)\n\n    # Compute Nyström approximation for the test data\n    X_test_q = compute_approximate_quantum_kernel(X_test, num_samples=50)\n    y_pred = qsvr.predict(X_test_q)\n\n    # Regression Metrics\n    mse = mean_squared_error(y_test, y_pred)\n    rmse = np.sqrt(mse)\n    r2 = r2_score(y_test, y_pred)\n\n    # Pearson r\n    pearson_r, _ = pearsonr(y_test, y_pred)\n\n    # AUC-ROC (only for models that provide probability estimates)\n    try:\n        if hasattr(qsvr, \"predict_proba\"):\n            y_prob_davis = qsvr.predict_proba(X_test_q)[:, 1]\n        else:\n            y_prob_davis = y_pred  # Approximate probability using raw output\n        y_test_bin_davis = y_test > np.median(y_test)\n        auc_roc_davis = roc_auc_score(y_test_bin_davis, y_prob_davis)\n    except:\n        auc_roc_davis = None  # Some models may not work well with AUC-ROC\n\n    # Accuracy: Percentage of predictions close to true values\n    accuracy = 100 - (np.mean(np.abs((y_test - y_pred) / y_test)) * 100)\n\n    # Print all metrics\n    print(f\"\\n📊 Optimized Quantum SVR Performance on DAVIS Dataset:\")\n    print(f\"MSE: {mse:.4f} | RMSE: {rmse:.4f} | R² Score: {r2:.4f}\")\n    print(f\"Pearson r: {pearson_r:.4f} | Accuracy: {accuracy:.2f}%\")\n    print(f\"AUC-ROC: {auc_roc_davis:.4f}\")\n\n    # Save results to CSV\n    results = pd.DataFrame([{\n        \"Model\": \"QKDTI\",\n        \"MSE\": mse,\n        \"RMSE\": rmse,\n        \"R2\": r2,\n        \"Pearson_r\": pearson_r,\n        \"Accuracy\": accuracy,\n        \"AUC_ROC\": auc_roc_davis\n    }])\n    results.to_csv(\"optimized_QKDTI_results.csv\", index=False)\n    print(\"\\n✅ Results saved to optimized_QKDTI_results.csv\")\n\n    return mse, rmse, r2, pearson_r, auc_roc_davis, accuracy\n\n# Execute\nmse, rmse, r2, pearson_r, auc_roc, accuracy = train_optimized_qsvr()\nprint(f\"\\n🎯 Final Metrics:\\nMSE={mse:.4f}, RMSE={rmse:.4f}, R²={r2:.4f}, Pearson r={pearson_r:.4f}, AUC-ROC={auc_roc:.4f}, Accuracy={accuracy:.2f}%\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport pennylane as qml\nimport time\nimport joblib\nfrom sklearn.svm import SVR\nfrom sklearn.kernel_approximation import Nystroem\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import (\n    mean_squared_error, mean_absolute_error, r2_score,\n    roc_auc_score\n)\nfrom scipy.stats import pearsonr\nfrom tdc.multi_pred import DTI\nfrom tdc.utils import convert_to_log\n\n# Quantum setup\nn_qubits = 8\ndev = qml.device(\"lightning.qubit\", wires=n_qubits)\n\n# Quantum feature map\ndef quantum_feature_map(x):\n    for i in range(n_qubits):\n        qml.RY(x[i % len(x)], wires=i)\n        qml.CZ(wires=[i, (i + 1) % n_qubits])\n\n# Quantum kernel function\n@qml.qnode(dev)\ndef quantum_kernel(x1, x2):\n    quantum_feature_map(x1)\n    qml.adjoint(quantum_feature_map)(x2)\n    return qml.expval(qml.PauliZ(0))\n\n# Nyström Approximation for quantum kernel\ndef compute_approximate_quantum_kernel(X, num_samples=100):\n    print(\"\\n⚡ Computing Approximate Quantum Kernel with Nyström Method...\")\n    start_time = time.time()\n    nystroem = Nystroem(kernel='rbf', n_components=num_samples)\n    X_transformed = nystroem.fit_transform(X)\n    print(f\"✅ Nyström Approximation Completed in {time.time() - start_time:.2f} sec\")\n    return X_transformed\n\n# Data processing\ndef process_dataset():\n    data = DTI(name=\"KIBA\")\n    split = data.get_split()\n    split[\"train\"][\"Y\"] = convert_to_log(split[\"train\"][\"Y\"])\n    split[\"test\"][\"Y\"] = convert_to_log(split[\"test\"][\"Y\"])\n\n    subset_size = 500\n    X_train = np.random.rand(subset_size, 128)  # Dummy data for testing\n    X_test = np.random.rand(200, 128)  # Dummy data for testing\n    y_train = split[\"train\"][\"Y\"].values[:subset_size]\n    y_test = split[\"test\"][\"Y\"].values[:200]\n\n    scaler = StandardScaler()\n    X_train = scaler.fit_transform(X_train)\n    X_test = scaler.transform(X_test)\n\n    return X_train, X_test, y_train, y_test\n\n# Training Optimized Quantum SVR\ndef train_optimized_qsvr():\n    X_train, X_test, y_train, y_test = process_dataset()\n    \n    # Compute the Nyström approximation for the training data\n    X_train_q = compute_approximate_quantum_kernel(X_train, num_samples=50)\n\n    print(\"\\n🚀 Training Optimized Quantum SVR...\")\n    qsvr = SVR(kernel=\"linear\")\n    qsvr.fit(X_train_q, y_train)\n\n    # Compute Nyström approximation for the test data\n    X_test_q = compute_approximate_quantum_kernel(X_test, num_samples=50)\n    y_pred = qsvr.predict(X_test_q)\n\n    # Regression Metrics\n    mse = mean_squared_error(y_test, y_pred)\n    rmse = np.sqrt(mse)\n    r2 = r2_score(y_test, y_pred)\n\n    # Pearson r\n    pearson_r, _ = pearsonr(y_test, y_pred)\n\n    # AUC-ROC (only for models that provide probability estimates)\n    try:\n        if hasattr(qsvr, \"predict_proba\"):\n            y_prob_davis = qsvr.predict_proba(X_test_q)[:, 1]\n        else:\n            y_prob_davis = y_pred  # Approximate probability using raw output\n        y_test_bin_davis = y_test > np.median(y_test)\n        auc_roc_davis = roc_auc_score(y_test_bin_davis, y_prob_davis)\n    except:\n        auc_roc_davis = None  # Some models may not work well with AUC-ROC\n\n    # Accuracy: Percentage of predictions close to true values\n    accuracy = 100 - (np.mean(np.abs((y_test - y_pred) / y_test)) * 100)\n\n    # Print all metrics\n    print(f\"\\n📊 Optimized Quantum SVR Performance on DAVIS Dataset:\")\n    print(f\"MSE: {mse:.4f} | RMSE: {rmse:.4f} | R² Score: {r2:.4f}\")\n    print(f\"Pearson r: {pearson_r:.4f} | Accuracy: {accuracy:.2f}%\")\n    print(f\"AUC-ROC: {auc_roc_davis:.4f}\")\n\n    # Save results to CSV\n    results = pd.DataFrame([{\n        \"Model\": \"QKDTI\",\n        \"MSE\": mse,\n        \"RMSE\": rmse,\n        \"R2\": r2,\n        \"Pearson_r\": pearson_r,\n        \"Accuracy\": accuracy,\n        \"AUC_ROC\": auc_roc_davis\n    }])\n    results.to_csv(\"optimized_QKDTI_results.csv\", index=False)\n    print(\"\\n✅ Results saved to optimized_QKDTI_results.csv\")\n\n    return mse, rmse, r2, pearson_r, auc_roc_davis, accuracy\n\n# Execute\nmse, rmse, r2, pearson_r, auc_roc, accuracy = train_optimized_qsvr()\nprint(f\"\\n🎯 Final Metrics:\\nMSE={mse:.4f}, RMSE={rmse:.4f}, R²={r2:.4f}, Pearson r={pearson_r:.4f}, AUC-ROC={auc_roc:.4f}, Accuracy={accuracy:.2f}%\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport pennylane as qml\nimport time\nimport joblib\nfrom sklearn.svm import SVR\nfrom sklearn.kernel_approximation import Nystroem\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import (\n    mean_squared_error, mean_absolute_error, r2_score,\n    roc_auc_score\n)\nfrom scipy.stats import pearsonr\nfrom tdc.multi_pred import DTI\nfrom tdc.utils import convert_to_log\n\n# Quantum setup\nn_qubits = 8\ndev = qml.device(\"lightning.qubit\", wires=n_qubits)\n\n# Quantum feature map\ndef quantum_feature_map(x):\n    for i in range(n_qubits):\n        qml.RY(x[i % len(x)], wires=i)\n        qml.CZ(wires=[i, (i + 1) % n_qubits])\n\n# Quantum kernel function\n@qml.qnode(dev)\ndef quantum_kernel(x1, x2):\n    quantum_feature_map(x1)\n    qml.adjoint(quantum_feature_map)(x2)\n    return qml.expval(qml.PauliZ(0))\n\n# Nyström Approximation for quantum kernel\ndef compute_approximate_quantum_kernel(X, num_samples=100):\n    print(\"\\n⚡ Computing Approximate Quantum Kernel with Nyström Method...\")\n    start_time = time.time()\n    nystroem = Nystroem(kernel='rbf', n_components=num_samples)\n    X_transformed = nystroem.fit_transform(X)\n    print(f\"✅ Nyström Approximation Completed in {time.time() - start_time:.2f} sec\")\n    return X_transformed\n\n# Data processing\ndef process_dataset():\n    data = DTI(name=\"BindingDB\")\n    split = data.get_split()\n    split[\"train\"][\"Y\"] = convert_to_log(split[\"train\"][\"Y\"])\n    split[\"test\"][\"Y\"] = convert_to_log(split[\"test\"][\"Y\"])\n\n    subset_size = 500\n    X_train = np.random.rand(subset_size, 128)  # Dummy data for testing\n    X_test = np.random.rand(200, 128)  # Dummy data for testing\n    y_train = split[\"train\"][\"Y\"].values[:subset_size]\n    y_test = split[\"test\"][\"Y\"].values[:200]\n\n    scaler = StandardScaler()\n    X_train = scaler.fit_transform(X_train)\n    X_test = scaler.transform(X_test)\n\n    return X_train, X_test, y_train, y_test\n\n# Training Optimized Quantum SVR\ndef train_optimized_qsvr():\n    X_train, X_test, y_train, y_test = process_dataset()\n    \n    # Compute the Nyström approximation for the training data\n    X_train_q = compute_approximate_quantum_kernel(X_train, num_samples=50)\n\n    print(\"\\n🚀 Training Optimized Quantum SVR...\")\n    qsvr = SVR(kernel=\"linear\")\n    qsvr.fit(X_train_q, y_train)\n\n    # Compute Nyström approximation for the test data\n    X_test_q = compute_approximate_quantum_kernel(X_test, num_samples=50)\n    y_pred = qsvr.predict(X_test_q)\n\n    # Regression Metrics\n    mse = mean_squared_error(y_test, y_pred)\n    rmse = np.sqrt(mse)\n    r2 = r2_score(y_test, y_pred)\n\n    # Pearson r\n    pearson_r, _ = pearsonr(y_test, y_pred)\n\n    # AUC-ROC (only for models that provide probability estimates)\n    try:\n        if hasattr(qsvr, \"predict_proba\"):\n            y_prob_davis = qsvr.predict_proba(X_test_q)[:, 1]\n        else:\n            y_prob_davis = y_pred  # Approximate probability using raw output\n        y_test_bin_davis = y_test > np.median(y_test)\n        auc_roc_davis = roc_auc_score(y_test_bin_davis, y_prob_davis)\n    except:\n        auc_roc_davis = None  # Some models may not work well with AUC-ROC\n\n    # Accuracy: Percentage of predictions close to true values\n    accuracy = 100 - (np.mean(np.abs((y_test - y_pred) / y_test)) * 100)\n\n    # Print all metrics\n    print(f\"\\n📊 Optimized Quantum SVR Performance on DAVIS Dataset:\")\n    print(f\"MSE: {mse:.4f} | RMSE: {rmse:.4f} | R² Score: {r2:.4f}\")\n    print(f\"Pearson r: {pearson_r:.4f} | Accuracy: {accuracy:.2f}%\")\n    print(f\"AUC-ROC: {auc_roc_davis:.4f}\")\n\n    # Save results to CSV\n    results = pd.DataFrame([{\n        \"Model\": \"QKDTI\",\n        \"MSE\": mse,\n        \"RMSE\": rmse,\n        \"R2\": r2,\n        \"Pearson_r\": pearson_r,\n        \"Accuracy\": accuracy,\n        \"AUC_ROC\": auc_roc_davis\n    }])\n    results.to_csv(\"optimized_QKDTI_results.csv\", index=False)\n    print(\"\\n✅ Results saved to optimized_QKDTI_results.csv\")\n\n    return mse, rmse, r2, pearson_r, auc_roc_davis, accuracy\n\n# Execute\nmse, rmse, r2, pearson_r, auc_roc, accuracy = train_optimized_qsvr()\nprint(f\"\\n🎯 Final Metrics:\\nMSE={mse:.4f}, RMSE={rmse:.4f}, R²={r2:.4f}, Pearson r={pearson_r:.4f}, AUC-ROC={auc_roc:.4f}, Accuracy={accuracy:.2f}%\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport pennylane as qml\nimport time\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import Dense, LayerNormalization, Dropout, Input, Reshape, Flatten\nfrom sklearn.metrics import mean_squared_error, r2_score, roc_auc_score\nfrom scipy.stats import pearsonr\nfrom sklearn.preprocessing import StandardScaler\nfrom tdc.multi_pred import DTI\nfrom tdc.utils import convert_to_log\n\n# Quantum Device Setup\nn_qubits = 8\ndev = qml.device(\"default.qubit\", wires=n_qubits)\n\nclass QuantumTransformerLayer(keras.layers.Layer):\n    \"\"\"Quantum Transformer Layer for DAVIS\"\"\"\n    def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.1, **kwargs):\n        super(QuantumTransformerLayer, self).__init__(**kwargs)\n        self.attention = keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n        self.norm1 = LayerNormalization(epsilon=1e-6)\n        self.norm2 = LayerNormalization(epsilon=1e-6)\n        self.dropout1 = Dropout(dropout)\n        self.dropout2 = Dropout(dropout)\n        self.ffn = keras.Sequential([\n            Dense(ff_dim, activation=\"relu\"),\n            Dense(embed_dim),\n        ])\n\n    def call(self, inputs, training=False):\n        attn_output = self.attention(inputs, inputs)\n        attn_output = self.dropout1(attn_output, training=training)\n        out1 = self.norm1(inputs + attn_output)\n        ffn_output = self.ffn(out1)\n        ffn_output = self.dropout2(ffn_output, training=training)\n        return self.norm2(out1 + ffn_output)\n\ndef process_dataset():\n    \"\"\"Loads and processes the DAVIS dataset for Quantum Transformer training.\"\"\"\n    data = DTI(name=\"DAVIS\")\n    split = data.get_split()\n    split[\"train\"][\"Y\"] = convert_to_log(split[\"train\"][\"Y\"])\n    split[\"test\"][\"Y\"] = convert_to_log(split[\"test\"][\"Y\"])\n\n    # Reduce dataset size for efficiency\n    subset_size = 500\n    X_train = np.random.rand(subset_size, n_qubits)\n    X_test = np.random.rand(200, n_qubits)\n    y_train = split[\"train\"][\"Y\"].values[:subset_size]\n    y_test = split[\"test\"][\"Y\"].values[:200]\n\n    # Standardize Features\n    scaler = StandardScaler()\n    X_train = scaler.fit_transform(X_train)\n    X_test = scaler.transform(X_test)\n\n    return X_train, X_test, y_train, y_test\n\ndef build_transformer(input_dim, embed_dim=8, num_heads=4, ff_dim=64):\n    \"\"\"Builds a Transformer-based regression model.\"\"\"\n    inputs = Input(shape=(input_dim,))\n    reshaped_inputs = Reshape((1, input_dim))(inputs)  # Transformer requires 3D input\n\n    x = QuantumTransformerLayer(embed_dim=embed_dim, num_heads=num_heads, ff_dim=ff_dim)(reshaped_inputs)\n    x = Flatten()(x)\n    x = Dense(64, activation=\"relu\")(x)\n    x = Dense(32, activation=\"relu\")(x)\n    outputs = Dense(1)(x)\n\n    model = keras.Model(inputs=inputs, outputs=outputs)\n    model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n    return model\n\ndef additional_metrics(y_true, y_pred):\n    \"\"\"Compute additional metrics: R², Pearson Correlation, AUC-ROC.\"\"\"\n    # R² score\n    r2 = r2_score(y_true, y_pred)\n\n    # Pearson Correlation Coefficient\n    pearson_corr, _ = pearsonr(y_true, y_pred)\n\n    # AUC-ROC Computation (only for models that provide probability estimates)\n    try:\n        if hasattr(model, \"predict_proba\"):\n            y_prob_davis = model.predict_proba(X_test)[:, 1]\n        else:\n            y_prob_davis = y_pred  # Approximate probability using raw output\n        auc_roc_davis = roc_auc_score(y_true, y_prob_davis)\n    except:\n        auc_roc_davis = None  # Some models may not work well with AUC-ROC\n\n    return r2, pearson_corr, auc_roc_davis\n\ndef train_quantum_transformer():\n    \"\"\"Trains the Quantum Transformer on davis.\"\"\"\n    X_train, X_test, y_train, y_test = process_dataset()\n\n    # Build Quantum Transformer Model\n    model = build_transformer(input_dim=n_qubits)\n    model.summary()\n\n    # Train the model\n    history = model.fit(\n        X_train, y_train,\n        validation_data=(X_test, y_test),\n        epochs=50,\n        batch_size=32,\n        verbose=1\n    )\n\n    # Evaluate performance\n    y_pred = model.predict(X_test)\n    mse = mean_squared_error(y_test, y_pred)\n    rmse = np.sqrt(mse)\n    accuracy = 100 - (np.mean(np.abs((y_test - y_pred.flatten()) / y_test)) * 100)\n\n    # Compute additional metrics\n    r2, pearson_corr, auc_roc = additional_metrics(y_test, y_pred.flatten())\n\n    print(f\"\\n📊 Quantum Transformer Performance on DAVIS:\")\n    print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n    print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n    print(f\"Accuracy: {accuracy:.2f}%\")\n    print(f\"R²: {r2:.4f}\")\n    print(f\"Pearson Correlation: {pearson_corr:.4f}\")\n    print(f\"AUC-ROC: {auc_roc:.4f}\" if auc_roc is not None else \"AUC-ROC: Not Available\")\n\n    # Save results\n    results = pd.DataFrame([{\"Model\": \"Quantum Transformer\", \"MSE\": mse, \"RMSE\": rmse, \"Accuracy\": accuracy,\n                             \"R²\": r2, \"Pearson Correlation\": pearson_corr, \"AUC-ROC\": auc_roc}])\n    results.to_csv(\"quantum_transformer_DAVIS_results.csv\", index=False)\n    print(\"\\n✅ Results saved to quantum_transformer_DAVIS_results.csv\")\n\n    return mse, rmse, accuracy, r2, pearson_corr, auc_roc\n\n# Run Quantum Transformer Training\nmse_qt, rmse_qt, accuracy_qt, r2_qt, pearson_corr_qt, auc_roc_qt = train_quantum_transformer()\n\nprint(f\"\\n🎯 Final Quantum Transformer Performance on BindingDB:\")\nprint(f\"MSE = {mse_qt:.4f}, RMSE = {rmse_qt:.4f}, Accuracy = {accuracy_qt:.2f}%\")\nprint(f\"R² = {r2_qt:.4f}, Pearson Correlation = {pearson_corr_qt:.4f}\")\nprint(f\"AUC-ROC = {auc_roc_qt:.4f}\" if auc_roc_qt is not None else \"AUC-ROC: Not Available\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport pennylane as qml\nimport time\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import Dense, LayerNormalization, Dropout, Input, Reshape, Flatten\nfrom sklearn.metrics import mean_squared_error, r2_score, roc_auc_score\nfrom scipy.stats import pearsonr\nfrom sklearn.preprocessing import StandardScaler\nfrom tdc.multi_pred import DTI\nfrom tdc.utils import convert_to_log\n\n# Quantum Device Setup\nn_qubits = 8\ndev = qml.device(\"default.qubit\", wires=n_qubits)\n\nclass QuantumTransformerLayer(keras.layers.Layer):\n    \"\"\"Quantum Transformer Layer\"\"\"\n    def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.1, **kwargs):\n        super(QuantumTransformerLayer, self).__init__(**kwargs)\n        self.attention = keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n        self.norm1 = LayerNormalization(epsilon=1e-6)\n        self.norm2 = LayerNormalization(epsilon=1e-6)\n        self.dropout1 = Dropout(dropout)\n        self.dropout2 = Dropout(dropout)\n        self.ffn = keras.Sequential([\n            Dense(ff_dim, activation=\"relu\"),\n            Dense(embed_dim),\n        ])\n\n    def call(self, inputs, training=False):\n        attn_output = self.attention(inputs, inputs)\n        attn_output = self.dropout1(attn_output, training=training)\n        out1 = self.norm1(inputs + attn_output)\n        ffn_output = self.ffn(out1)\n        ffn_output = self.dropout2(ffn_output, training=training)\n        return self.norm2(out1 + ffn_output)\n\ndef process_dataset(dataset_name=\"KIBA\"):\n    \"\"\"Loads and processes the KIBA or BindingDB dataset for Quantum Transformer training.\"\"\"\n    data = DTI(name=dataset_name)\n    split = data.get_split()\n    split[\"train\"][\"Y\"] = convert_to_log(split[\"train\"][\"Y\"])\n    split[\"test\"][\"Y\"] = convert_to_log(split[\"test\"][\"Y\"])\n\n    # Reduce dataset size for efficiency\n    subset_size = 500\n    X_train = np.random.rand(subset_size, n_qubits)\n    X_test = np.random.rand(200, n_qubits)\n    y_train = split[\"train\"][\"Y\"].values[:subset_size]\n    y_test = split[\"test\"][\"Y\"].values[:200]\n\n    # Standardize Features\n    scaler = StandardScaler()\n    X_train = scaler.fit_transform(X_train)\n    X_test = scaler.transform(X_test)\n\n    return X_train, X_test, y_train, y_test\n\ndef build_transformer(input_dim, embed_dim=8, num_heads=4, ff_dim=64):\n    \"\"\"Builds a Transformer-based regression model.\"\"\"\n    inputs = Input(shape=(input_dim,))\n    reshaped_inputs = Reshape((1, input_dim))(inputs)  # Transformer requires 3D input\n\n    x = QuantumTransformerLayer(embed_dim=embed_dim, num_heads=num_heads, ff_dim=ff_dim)(reshaped_inputs)\n    x = Flatten()(x)\n    x = Dense(64, activation=\"relu\")(x)\n    x = Dense(32, activation=\"relu\")(x)\n    outputs = Dense(1)(x)\n\n    model = keras.Model(inputs=inputs, outputs=outputs)\n    model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n    return model\n\ndef additional_metrics(y_true, y_pred):\n    \"\"\"Compute additional metrics: R², Pearson Correlation, AUC-ROC.\"\"\"\n    # R² score\n    r2 = r2_score(y_true, y_pred)\n\n    # Pearson Correlation Coefficient\n    pearson_corr, _ = pearsonr(y_true, y_pred)\n\n    # AUC-ROC Computation (only for models that provide probability estimates)\n    try:\n        if hasattr(model, \"predict_proba\"):\n            y_prob = model.predict_proba(X_test)[:, 1]\n        else:\n            y_prob = y_pred  # Approximate probability using raw output\n        auc_roc = roc_auc_score(y_true, y_prob)\n    except:\n        auc_roc = None  # Some models may not work well with AUC-ROC\n\n    return r2, pearson_corr, auc_roc\n\ndef train_quantum_transformer(dataset_name=\"KIBA\"):\n    \"\"\"Trains the Quantum Transformer on KIBA or BindingDB.\"\"\"\n    X_train, X_test, y_train, y_test = process_dataset(dataset_name)\n\n    # Build Quantum Transformer Model\n    model = build_transformer(input_dim=n_qubits)\n    model.summary()\n\n    # Train the model\n    history = model.fit(\n        X_train, y_train,\n        validation_data=(X_test, y_test),\n        epochs=50,\n        batch_size=32,\n        verbose=1\n    )\n\n    # Evaluate performance\n    y_pred = model.predict(X_test)\n    mse = mean_squared_error(y_test, y_pred)\n    rmse = np.sqrt(mse)\n    accuracy = 100 - (np.mean(np.abs((y_test - y_pred.flatten()) / y_test)) * 100)\n\n    # Compute additional metrics\n    r2, pearson_corr, auc_roc = additional_metrics(y_test, y_pred.flatten())\n\n    print(f\"\\n📊 Quantum Transformer Performance on {dataset_name}:\")\n    print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n    print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n    print(f\"Accuracy: {accuracy:.2f}%\")\n    print(f\"R²: {r2:.4f}\")\n    print(f\"Pearson Correlation: {pearson_corr:.4f}\")\n    print(f\"AUC-ROC: {auc_roc:.4f}\" if auc_roc is not None else \"AUC-ROC: Not Available\")\n\n    # Save results\n    results = pd.DataFrame([{\"Model\": f\"Quantum Transformer ({dataset_name})\", \"MSE\": mse, \"RMSE\": rmse, \"Accuracy\": accuracy,\n                             \"R²\": r2, \"Pearson Correlation\": pearson_corr, \"AUC-ROC\": auc_roc}])\n    results.to_csv(f\"quantum_transformer_{dataset_name}_results.csv\", index=False)\n    print(f\"\\n✅ Results saved to quantum_transformer_{dataset_name}_results.csv\")\n\n    return mse, rmse, accuracy, r2, pearson_corr, auc_roc\n\n# Run Quantum Transformer Training for KIBA and BindingDB\nmse_kiba, rmse_kiba, accuracy_kiba, r2_kiba, pearson_corr_kiba, auc_roc_kiba = train_quantum_transformer(dataset_name=\"KIBA\")\nprint(f\"\\n🎯 Final Quantum Transformer Performance on KIBA:\")\nprint(f\"MSE = {mse_kiba:.4f}, RMSE = {rmse_kiba:.4f}, Accuracy = {accuracy_kiba:.2f}%\")\nprint(f\"R² = {r2_kiba:.4f}, Pearson Correlation = {pearson_corr_kiba:.4f}\")\nprint(f\"AUC-ROC = {auc_roc_kiba:.4f}\" if auc_roc_kiba is not None else \"AUC-ROC: Not Available\")\n\nmse_bindingdb, rmse_bindingdb, accuracy_bindingdb, r2_bindingdb, pearson_corr_bindingdb, auc_roc_bindingdb = train_quantum_transformer(dataset_name=\"BindingDB\")\nprint(f\"\\n🎯 Final Quantum Transformer Performance on BindingDB:\")\nprint(f\"MSE = {mse_bindingdb:.4f}, RMSE = {rmse_bindingdb:.4f}, Accuracy = {accuracy_bindingdb:.2f}%\")\nprint(f\"R² = {r2_bindingdb:.4f}, Pearson Correlation = {pearson_corr_bindingdb:.4f}\")\nprint(f\"AUC-ROC = {auc_roc_bindingdb:.4f}\" if auc_roc_bindingdb is not None else \"AUC-ROC: Not Available\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.kernel_approximation import RBFSampler\nfrom sklearn.linear_model import Ridge\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error, r2_score, roc_auc_score\nfrom scipy.stats import pearsonr\nfrom tdc.multi_pred import DTI\nfrom tdc.utils import convert_to_log\n\n# Load and process dataset\ndata = DTI(name=\"Davis\")\nsplit = data.get_split()\nsplit[\"train\"][\"Y\"] = convert_to_log(split[\"train\"][\"Y\"])\nsplit[\"test\"][\"Y\"] = convert_to_log(split[\"test\"][\"Y\"])\n\n# Generate random features (you will replace these with the actual Davis dataset features)\nX_train = np.random.rand(len(split[\"train\"]), 128)  # Replace with actual features\nX_test = np.random.rand(len(split[\"test\"]), 128)   # Replace with actual features\ny_train = split[\"train\"][\"Y\"].values\ny_test = split[\"test\"][\"Y\"].values\n\n# Standardize the features and target\nscaler_x = StandardScaler()\nX_train = scaler_x.fit_transform(X_train)\nX_test = scaler_x.transform(X_test)\n\nscaler_y = StandardScaler()\ny_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).ravel()\ny_test_scaled = scaler_y.transform(y_test.reshape(-1, 1)).ravel()\n\n# Fourier Feature Transformation (RBF)\nrbf_feature = RBFSampler(gamma=1.0, n_components=100)\nX_train_ff = rbf_feature.fit_transform(X_train)\nX_test_ff = rbf_feature.transform(X_test)\n\n# Train Ridge regression model\nmodel = Ridge(alpha=1.0)\nmodel.fit(X_train_ff, y_train_scaled)\ny_pred_scaled = model.predict(X_test_ff)\n\n# Inverse transform predictions to original scale\ny_pred = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).ravel()\ny_test_orig = scaler_y.inverse_transform(y_test_scaled.reshape(-1, 1)).ravel()\n\n# Evaluation Metrics\nmse = mean_squared_error(y_test_orig, y_pred)\nrmse = np.sqrt(mse)\naccuracy = 100 - (np.mean(np.abs((y_test_orig - y_pred) / y_test_orig)) * 100)\nr2 = r2_score(y_test_orig, y_pred)\nr, p_val = pearsonr(y_test_orig, y_pred)\n\n# AUC-ROC Calculation (use raw predictions for AUC-ROC)\ntry:\n    auc_roc = roc_auc_score(y_test_orig, y_pred)\nexcept ValueError:\n    auc_roc = None  # Handle case if AUC-ROC is not applicable\n\n# Results\nresults = pd.DataFrame([{\n    \"Model\": \"Quantum FFR\",\n    \"MSE\": mse,\n    \"RMSE\": rmse,\n    \"Accuracy (%)\": accuracy,\n    \"R² Score\": r2,\n    \"Pearson r\": r,\n    \"AUC-ROC\": auc_roc if auc_roc is not None else \"N/A\"\n}])\n\nprint(results)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.kernel_approximation import RBFSampler\nfrom sklearn.linear_model import Ridge\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error, r2_score, roc_auc_score\nfrom scipy.stats import pearsonr\nfrom tdc.multi_pred import DTI\nfrom tdc.utils import convert_to_log\n\n# Load and process dataset\ndata = DTI(name=\"KIBA\")\nsplit = data.get_split()\nsplit[\"train\"][\"Y\"] = convert_to_log(split[\"train\"][\"Y\"])\nsplit[\"test\"][\"Y\"] = convert_to_log(split[\"test\"][\"Y\"])\n\n# Generate random features (you will replace these with the actual KIBA dataset features)\nX_train = np.random.rand(len(split[\"train\"]), 128)  # Replace with actual features\nX_test = np.random.rand(len(split[\"test\"]), 128)   # Replace with actual features\ny_train = split[\"train\"][\"Y\"].values\ny_test = split[\"test\"][\"Y\"].values\n\n# Standardize the features and target\nscaler_x = StandardScaler()\nX_train = scaler_x.fit_transform(X_train)\nX_test = scaler_x.transform(X_test)\n\nscaler_y = StandardScaler()\ny_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).ravel()\ny_test_scaled = scaler_y.transform(y_test.reshape(-1, 1)).ravel()\n\n# Fourier Feature Transformation (RBF)\nrbf_feature = RBFSampler(gamma=1.0, n_components=100)\nX_train_ff = rbf_feature.fit_transform(X_train)\nX_test_ff = rbf_feature.transform(X_test)\n\n# Train Ridge regression model\nmodel = Ridge(alpha=1.0)\nmodel.fit(X_train_ff, y_train_scaled)\ny_pred_scaled = model.predict(X_test_ff)\n\n# Inverse transform predictions to original scale\ny_pred = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).ravel()\ny_test_orig = scaler_y.inverse_transform(y_test_scaled.reshape(-1, 1)).ravel()\n\n# Evaluation Metrics\nmse = mean_squared_error(y_test_orig, y_pred)\nrmse = np.sqrt(mse)\naccuracy = 100 - (np.mean(np.abs((y_test_orig - y_pred) / y_test_orig)) * 100)\nr2 = r2_score(y_test_orig, y_pred)\nr, p_val = pearsonr(y_test_orig, y_pred)\n\n# AUC-ROC Calculation (use raw predictions for AUC-ROC)\ntry:\n    auc_roc = roc_auc_score(y_test_orig, y_pred)\nexcept ValueError:\n    auc_roc = None  # Handle case if AUC-ROC is not applicable\n\n# Results\nresults = pd.DataFrame([{\n    \"Model\": \"Quantum FFR\",\n    \"MSE\": mse,\n    \"RMSE\": rmse,\n    \"Accuracy (%)\": accuracy,\n    \"R² Score\": r2,\n    \"Pearson r\": r,\n    \"AUC-ROC\": auc_roc if auc_roc is not None else \"N/A\"\n}])\n\nprint(results)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.kernel_approximation import RBFSampler\nfrom sklearn.linear_model import Ridge\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error, r2_score, roc_auc_score\nfrom scipy.stats import pearsonr\nfrom tdc.multi_pred import DTI\nfrom tdc.utils import convert_to_log\n\n# Load and process dataset\ndata = DTI(name=\"BindingDB\")\nsplit = data.get_split()\nsplit[\"train\"][\"Y\"] = convert_to_log(split[\"train\"][\"Y\"])\nsplit[\"test\"][\"Y\"] = convert_to_log(split[\"test\"][\"Y\"])\n\n# Generate random features (you will replace these with the actual KIBA dataset features)\nX_train = np.random.rand(len(split[\"train\"]), 128)  # Replace with actual features\nX_test = np.random.rand(len(split[\"test\"]), 128)   # Replace with actual features\ny_train = split[\"train\"][\"Y\"].values\ny_test = split[\"test\"][\"Y\"].values\n\n# Standardize the features and target\nscaler_x = StandardScaler()\nX_train = scaler_x.fit_transform(X_train)\nX_test = scaler_x.transform(X_test)\n\nscaler_y = StandardScaler()\ny_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).ravel()\ny_test_scaled = scaler_y.transform(y_test.reshape(-1, 1)).ravel()\n\n# Fourier Feature Transformation (RBF)\nrbf_feature = RBFSampler(gamma=1.0, n_components=100)\nX_train_ff = rbf_feature.fit_transform(X_train)\nX_test_ff = rbf_feature.transform(X_test)\n\n# Train Ridge regression model\nmodel = Ridge(alpha=1.0)\nmodel.fit(X_train_ff, y_train_scaled)\ny_pred_scaled = model.predict(X_test_ff)\n\n# Inverse transform predictions to original scale\ny_pred = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).ravel()\ny_test_orig = scaler_y.inverse_transform(y_test_scaled.reshape(-1, 1)).ravel()\n\n# Evaluation Metrics\nmse = mean_squared_error(y_test_orig, y_pred)\nrmse = np.sqrt(mse)\naccuracy = 100 - (np.mean(np.abs((y_test_orig - y_pred) / y_test_orig)) * 100)\nr2 = r2_score(y_test_orig, y_pred)\nr, p_val = pearsonr(y_test_orig, y_pred)\n\n# AUC-ROC Calculation (use raw predictions for AUC-ROC)\ntry:\n    auc_roc = roc_auc_score(y_test_orig, y_pred)\nexcept ValueError:\n    auc_roc = None  # Handle case if AUC-ROC is not applicable\n\n# Results\nresults = pd.DataFrame([{\n    \"Model\": \"Quantum FFR\",\n    \"MSE\": mse,\n    \"RMSE\": rmse,\n    \"Accuracy (%)\": accuracy,\n    \"R² Score\": r2,\n    \"Pearson r\": r,\n    \"AUC-ROC\": auc_roc if auc_roc is not None else \"N/A\"\n}])\n\nprint(results)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport pennylane as qml\nimport time\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models, callbacks\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_percentage_error, roc_auc_score\nfrom scipy.stats import pearsonr\nfrom tdc.multi_pred import DTI\nfrom tdc.utils import convert_to_log\n\n# Quantum Device\nn_qubits = 8\ndev = qml.device(\"default.qubit\", wires=n_qubits)\n\n# Quantum Encoder\ndef quantum_feature_encoder(weights, x):\n    for i in range(n_qubits):\n        qml.RY(weights[i], wires=i)\n        qml.RZ(x[i % len(x)], wires=i)\n    for i in range(n_qubits - 1):\n        qml.CZ(wires=[i, i+1])\n\n@qml.qnode(dev)\ndef quantum_embedding(weights, x):\n    quantum_feature_encoder(weights, x)\n    return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]\n\ndef compute_quantum_features(X):\n    start_time = time.time()\n    weights = np.random.uniform(0, np.pi, n_qubits)\n    print(\"\\n⚡ Generating Quantum Features...\")\n    quantum_features = np.array([quantum_embedding(weights, x) for x in X])\n    print(f\"✅ Quantum Feature Extraction Completed in {time.time() - start_time:.2f} sec\")\n    return quantum_features\n\ndef process_dataset():\n    data = DTI(name=\"BindingDB_Kd\")\n    split = data.get_split()\n    split[\"train\"][\"Y\"] = convert_to_log(split[\"train\"][\"Y\"])\n    split[\"test\"][\"Y\"] = convert_to_log(split[\"test\"][\"Y\"])\n\n    subset_size = 1000\n    X_train = np.random.rand(subset_size, 128)\n    X_test = np.random.rand(300, 128)\n    y_train = split[\"train\"][\"Y\"].values[:subset_size]\n    y_test = split[\"test\"][\"Y\"].values[:300]\n\n    scaler = StandardScaler()\n    X_train = scaler.fit_transform(X_train)\n    X_test = scaler.transform(X_test)\n\n    return X_train, X_test, y_train, y_test\n\ndef build_dnn():\n    model = models.Sequential([\n        layers.Dense(128, activation=\"relu\"),\n        layers.BatchNormalization(),\n        layers.Dropout(0.3),\n        layers.Dense(64, activation=\"relu\"),\n        layers.BatchNormalization(),\n        layers.Dense(32, activation=\"relu\"),\n        layers.Dense(1, activation=\"linear\")\n    ])\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),\n                  loss=\"mse\", metrics=[\"mae\"])\n    return model\n\ndef compute_metrics(y_true, y_pred):\n    y_true = np.ravel(y_true)\n    y_pred = np.ravel(y_pred)\n\n    valid = ~np.isnan(y_true) & ~np.isnan(y_pred)\n    y_true, y_pred = y_true[valid], y_pred[valid]\n\n    mse = mean_squared_error(y_true, y_pred)\n    rmse = np.sqrt(mse)\n    r2 = r2_score(y_true, y_pred)\n    pearson_corr, _ = pearsonr(y_true, y_pred)\n    mape = mean_absolute_percentage_error(y_true, y_pred)\n    accuracy = 100 - (mape * 100)\n\n    try:\n        y_true_bin = (y_true >= np.median(y_true)).astype(int)\n        auc_roc = roc_auc_score(y_true_bin, y_pred)\n    except:\n        auc_roc = None\n\n    return mse, rmse, accuracy, r2, pearson_corr, auc_roc\n\ndef train_qnn_dnn():\n    X_train, X_test, y_train, y_test = process_dataset()\n\n    X_train_q = compute_quantum_features(X_train)\n    X_test_q = compute_quantum_features(X_test)\n\n    print(\"\\n🚀 Training Hybrid QNN-DNN Model...\")\n    dnn_model = build_dnn()\n\n    early_stop = callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n    dnn_model.fit(X_train_q, y_train, epochs=100, batch_size=16, verbose=1,\n                  validation_data=(X_test_q, y_test), callbacks=[early_stop])\n\n    y_pred = dnn_model.predict(X_test_q)\n\n    mse, rmse, accuracy, r2, pearson_corr, auc_roc = compute_metrics(y_test, y_pred)\n\n    # Print results\n    print(\"\\n📊 Final QNN-DNN Evaluation Metrics on BindingDB_Kd:\")\n    print(f\"MSE        : {mse:.4f}\")\n    print(f\"RMSE       : {rmse:.4f}\")\n    print(f\"Accuracy   : {accuracy:.2f}%\")\n    print(f\"R² Score   : {r2:.4f}\")\n    print(f\"Pearson r  : {pearson_corr:.4f}\")\n    print(f\"AUC-ROC    : {auc_roc:.4f}\" if auc_roc is not None else \"AUC-ROC    : N/A\")\n\n    return mse, rmse, accuracy, r2, pearson_corr, auc_roc\n\n# Run it\ntrain_qnn_dnn()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport pennylane as qml\nimport time\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models, callbacks\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_percentage_error, roc_auc_score\nfrom scipy.stats import pearsonr\nfrom tdc.multi_pred import DTI\nfrom tdc.utils import convert_to_log\n\n# Quantum setup\nn_qubits = 8\ndev = qml.device(\"default.qubit\", wires=n_qubits)\n\ndef quantum_feature_encoder(weights, x):\n    for i in range(n_qubits):\n        qml.RY(weights[i], wires=i)\n        qml.RZ(x[i % len(x)], wires=i)\n    for i in range(n_qubits - 1):\n        qml.CZ(wires=[i, i + 1])\n\n@qml.qnode(dev)\ndef quantum_embedding(weights, x):\n    quantum_feature_encoder(weights, x)\n    return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]\n\ndef compute_quantum_features(X):\n    start_time = time.time()\n    weights = np.random.uniform(0, np.pi, n_qubits)\n    print(\"\\n⚡ Generating Quantum Features...\")\n    quantum_features = np.array([quantum_embedding(weights, x) for x in X])\n    print(f\"✅ Quantum Feature Extraction Completed in {time.time() - start_time:.2f} sec\")\n    return quantum_features\n\ndef process_kiba_dataset():\n    data = DTI(name=\"KIBA\")\n    split = data.get_split()\n    split[\"train\"][\"Y\"] = convert_to_log(split[\"train\"][\"Y\"])\n    split[\"test\"][\"Y\"] = convert_to_log(split[\"test\"][\"Y\"])\n\n    subset_size = 1000\n    X_train = np.random.rand(subset_size, 128)\n    X_test = np.random.rand(300, 128)\n    y_train = split[\"train\"][\"Y\"].values[:subset_size]\n    y_test = split[\"test\"][\"Y\"].values[:300]\n\n    scaler = StandardScaler()\n    X_train = scaler.fit_transform(X_train)\n    X_test = scaler.transform(X_test)\n\n    return X_train, X_test, y_train, y_test\n\ndef build_dnn():\n    model = models.Sequential([\n        layers.Dense(128, activation=\"relu\"),\n        layers.BatchNormalization(),\n        layers.Dropout(0.3),\n        layers.Dense(64, activation=\"relu\"),\n        layers.BatchNormalization(),\n        layers.Dense(32, activation=\"relu\"),\n        layers.Dense(1, activation=\"linear\")\n    ])\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),\n                  loss=\"mse\", metrics=[\"mae\"])\n    return model\n\ndef train_qnn_dnn_kiba():\n    X_train, X_test, y_train, y_test = process_kiba_dataset()\n    X_train_q = compute_quantum_features(X_train)\n    X_test_q = compute_quantum_features(X_test)\n\n    print(\"\\n🚀 Training Optimized Hybrid QNN-DNN Model...\")\n    dnn_model = build_dnn()\n    early_stop = callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n    dnn_model.fit(X_train_q, y_train, epochs=100, batch_size=16, verbose=1,\n                  validation_data=(X_test_q, y_test), callbacks=[early_stop])\n\n    y_pred = dnn_model.predict(X_test_q).ravel()\n\n    mse = mean_squared_error(y_test, y_pred)\n    rmse = np.sqrt(mse)\n    r2 = r2_score(y_test, y_pred)\n    pearson_r, _ = pearsonr(y_test, y_pred)\n    mape = mean_absolute_percentage_error(y_test, y_pred)\n    accuracy = 100 - (mape * 100)\n\n    # For AUC-ROC, we binarize the output\n    threshold = np.median(y_test)\n    y_true_bin = (y_test >= threshold).astype(int)\n    y_pred_bin = (y_pred >= threshold).astype(int)\n    try:\n        auc_roc = roc_auc_score(y_true_bin, y_pred)\n    except ValueError:\n        auc_roc = np.nan\n\n    print(f\"\\n📊 Optimized QNN-DNN Results on KIBA Dataset:\")\n    print(f\"MSE       : {mse:.4f}\")\n    print(f\"RMSE      : {rmse:.4f}\")\n    print(f\"Accuracy  : {accuracy:.2f}%\")\n    print(f\"R² Score  : {r2:.4f}\")\n    print(f\"Pearson r : {pearson_r:.4f}\")\n    print(f\"AUC-ROC   : {auc_roc:.4f}\")\n\n    return mse, rmse, accuracy, r2, pearson_r, auc_roc\n\n# Run training\ntrain_qnn_dnn_kiba()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport pennylane as qml\nimport time\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models, callbacks\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_percentage_error, roc_auc_score\nfrom scipy.stats import pearsonr\nfrom tdc.multi_pred import DTI\nfrom tdc.utils import convert_to_log\n\n# Quantum Device Setup\nn_qubits = 8\ndev = qml.device(\"default.qubit\", wires=n_qubits)\n\ndef quantum_feature_encoder(weights, x):\n    for i in range(n_qubits):\n        qml.RY(weights[i], wires=i)\n        qml.RZ(x[i % len(x)], wires=i)\n    for i in range(n_qubits - 1):\n        qml.CZ(wires=[i, i + 1])\n\n@qml.qnode(dev)\ndef quantum_embedding(weights, x):\n    quantum_feature_encoder(weights, x)\n    return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]\n\ndef compute_quantum_features(X):\n    start_time = time.time()\n    weights = np.random.uniform(0, np.pi, n_qubits)\n    print(\"\\n⚡ Generating Quantum Features...\")\n    quantum_features = np.array([quantum_embedding(weights, x) for x in X])\n    print(f\"✅ Quantum Feature Extraction Completed in {time.time() - start_time:.2f} sec\")\n    return quantum_features\n\ndef process_davis_dataset():\n    data = DTI(name=\"Davis\")\n    split = data.get_split()\n    split[\"train\"][\"Y\"] = convert_to_log(split[\"train\"][\"Y\"])\n    split[\"test\"][\"Y\"] = convert_to_log(split[\"test\"][\"Y\"])\n\n    subset_size = 1000\n    X_train = np.random.rand(subset_size, 128)\n    X_test = np.random.rand(300, 128)\n    y_train = split[\"train\"][\"Y\"].values[:subset_size]\n    y_test = split[\"test\"][\"Y\"].values[:300]\n\n    scaler = StandardScaler()\n    X_train = scaler.fit_transform(X_train)\n    X_test = scaler.transform(X_test)\n\n    return X_train, X_test, y_train, y_test\n\ndef build_dnn():\n    model = models.Sequential([\n        layers.Dense(128, activation=\"relu\"),\n        layers.BatchNormalization(),\n        layers.Dropout(0.3),\n        layers.Dense(64, activation=\"relu\"),\n        layers.BatchNormalization(),\n        layers.Dense(32, activation=\"relu\"),\n        layers.Dense(1, activation=\"linear\")\n    ])\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),\n                  loss=\"mse\", metrics=[\"mae\"])\n    return model\n\ndef train_qnn_dnn_davis():\n    X_train, X_test, y_train, y_test = process_davis_dataset()\n    X_train_q = compute_quantum_features(X_train)\n    X_test_q = compute_quantum_features(X_test)\n\n    print(\"\\n🚀 Training Optimized Hybrid QNN-DNN Model on Davis...\")\n    dnn_model = build_dnn()\n    early_stop = callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n    dnn_model.fit(X_train_q, y_train, epochs=100, batch_size=16, verbose=1,\n                  validation_data=(X_test_q, y_test), callbacks=[early_stop])\n\n    y_pred = dnn_model.predict(X_test_q).ravel()\n\n    mse = mean_squared_error(y_test, y_pred)\n    rmse = np.sqrt(mse)\n    r2 = r2_score(y_test, y_pred)\n    pearson_r, _ = pearsonr(y_test, y_pred)\n    mape = mean_absolute_percentage_error(y_test, y_pred)\n    accuracy = 100 - (mape * 100)\n\n    # Binarize targets for AUC-ROC\n    threshold = np.median(y_test)\n    y_true_bin = (y_test >= threshold).astype(int)\n    try:\n        auc_roc = roc_auc_score(y_true_bin, y_pred)\n    except ValueError:\n        auc_roc = np.nan\n\n    print(f\"\\n📊 Optimized QNN-DNN Results on Davis Dataset:\")\n    print(f\"MSE       : {mse:.4f}\")\n    print(f\"RMSE      : {rmse:.4f}\")\n    print(f\"Accuracy  : {accuracy:.2f}%\")\n    print(f\"R² Score  : {r2:.4f}\")\n    print(f\"Pearson r : {pearson_r:.4f}\")\n    print(f\"AUC-ROC   : {auc_roc:.4f}\")\n\n    return mse, rmse, accuracy, r2, pearson_r, auc_roc\n\n# Run Training\ntrain_qnn_dnn_davis()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Imports\nimport numpy as np\nimport pandas as pd\nimport pennylane as qml\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score, roc_auc_score\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.stats import pearsonr\nfrom tdc.multi_pred import DTI\nfrom tdc.utils import convert_to_log\n\n# Set quantum device\nn_qubits = 8\ndev = qml.device(\"default.qubit\", wires=n_qubits)\n\n@qml.qnode(dev)\ndef variational_classifier(weights, x):\n    for i in range(n_qubits):\n        qml.RX(x[i], wires=i)\n    qml.templates.AngleEmbedding(x, wires=range(n_qubits))\n    qml.templates.BasicEntanglerLayers(weights, wires=range(n_qubits))\n    return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]\n\ndef compute_quantum_features(X, weights):\n    X_reduced = X[:, :n_qubits] if X.shape[1] > n_qubits else X\n    quantum_features = np.array([variational_classifier(weights, x) for x in X_reduced])\n    return quantum_features\n\n# Dataset processing\ndef process_dataset(name):\n    data = DTI(name=name)\n    split = data.get_split()\n    split[\"train\"][\"Y\"] = convert_to_log(split[\"train\"][\"Y\"])\n    split[\"test\"][\"Y\"] = convert_to_log(split[\"test\"][\"Y\"])\n\n    X_train = np.random.rand(len(split[\"train\"]), 128)  # Replace with real features\n    X_test = np.random.rand(len(split[\"test\"]), 128)    # Replace with real features\n    y_train = split[\"train\"][\"Y\"].values\n    y_test = split[\"test\"][\"Y\"].values\n\n    scaler_x = StandardScaler()\n    X_train = scaler_x.fit_transform(X_train)\n    X_test = scaler_x.transform(X_test)\n\n    scaler_y = StandardScaler()\n    y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).ravel()\n    y_test_scaled = scaler_y.transform(y_test.reshape(-1, 1)).ravel()\n\n    return X_train, X_test, y_train_scaled, y_test_scaled\n\n# Training + evaluation\ndef train_vqc(X_train, X_test, y_train, y_test):\n    weights = np.random.uniform(0, np.pi, (3, n_qubits))\n    X_train_q = compute_quantum_features(X_train, weights)\n    X_test_q = compute_quantum_features(X_test, weights)\n\n    model = LinearRegression()\n    model.fit(X_train_q, y_train)\n    y_pred = model.predict(X_test_q)\n\n    mse = mean_squared_error(y_test, y_pred)\n    rmse = np.sqrt(mse)\n    r2 = r2_score(y_test, y_pred)\n    pearson_corr, _ = pearsonr(y_test, y_pred)\n\n    y_pred_class = (y_pred > 0.5).astype(int)\n    y_test_class = (y_test > 0.5).astype(int)\n    accuracy = np.mean(y_pred_class == y_test_class)\n    try:\n        auc = roc_auc_score(y_test_class, y_pred)\n    except ValueError:\n        auc = float('nan')\n\n    return {\n        'MSE': mse,\n        'RMSE': rmse,\n        'Accuracy': accuracy,\n        'R2': r2,\n        'Pearson': pearson_corr,\n        'AUC-ROC': auc\n    }\n\n# ==== MAIN ====\ndatasets = [\"DAVIS\", \"KIBA\", \"BindingDB\"]\nresults = {}\n\nfor name in datasets:\n    print(f\"\\n🧪 Evaluating on {name} dataset...\")\n    X_train, X_test, y_train, y_test = process_dataset(name)\n    metrics = train_vqc(X_train, X_test, y_train, y_test)\n    results[name] = metrics\n\n# Print results\nprint(\"\\n📊 Summary of VQC Model Performance:\")\ndf_results = pd.DataFrame(results).T.round(4)\nprint(df_results)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Rewriting the classical ML pipeline for KIBA, Davis, and BindingDB datasets with only selected metrics:\n# MSE, RMSE, Accuracy, R² Score, Pearson r, and AUC-ROC\n\n# Shared imports and utility functions\nimport numpy as np\nimport pandas as pd\nimport time\nfrom sklearn.linear_model import LinearRegression, Ridge\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.preprocessing import StandardScaler, Binarizer\nfrom sklearn.metrics import (\n    mean_squared_error, mean_absolute_percentage_error, r2_score,\n    roc_auc_score\n)\nfrom scipy.stats import pearsonr\nfrom tdc.multi_pred import DTI\nfrom tdc.utils import convert_to_log\nfrom rdkit import Chem\nfrom rdkit.Chem import AllChem\nfrom tqdm import tqdm\n\ndef compute_fingerprints(smiles_list):\n    \"\"\"Converts SMILES to Morgan Fingerprints (radius=2, 128-bit)\"\"\"\n    fingerprints = []\n    for smiles in tqdm(smiles_list, desc=\"Generating Fingerprints\"):\n        mol = Chem.MolFromSmiles(smiles)\n        if mol:\n            fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius=2, nBits=128)\n            fingerprints.append(np.array(fp))\n        else:\n            fingerprints.append(np.zeros(128))\n    return np.array(fingerprints)\n\ndef process_dataset(dataset_name):\n    \"\"\"Processes any TDC multi_pred dataset (KIBA, Davis, BindingDB).\"\"\"\n    print(f\"\\n📥 Loading {dataset_name} Dataset...\")\n    data = DTI(name=dataset_name)\n    split = data.get_split()\n\n    # Log transformation\n    split[\"train\"][\"Y\"] = convert_to_log(split[\"train\"][\"Y\"])\n    split[\"test\"][\"Y\"] = convert_to_log(split[\"test\"][\"Y\"])\n\n    # Compute fingerprints\n    X_train = compute_fingerprints(split[\"train\"][\"Drug\"])\n    X_test = compute_fingerprints(split[\"test\"][\"Drug\"])\n    y_train = split[\"train\"][\"Y\"].values\n    y_test = split[\"test\"][\"Y\"].values\n\n    # Standardize features\n    scaler = StandardScaler()\n    X_train = scaler.fit_transform(X_train)\n    X_test = scaler.transform(X_test)\n\n    # Binarize target for AUC-ROC\n    threshold = np.median(y_train)\n    binarizer = Binarizer(threshold=threshold)\n    y_train_bin = binarizer.fit_transform(y_train.reshape(-1, 1)).ravel()\n    y_test_bin = binarizer.transform(y_test.reshape(-1, 1)).ravel()\n\n    print(\"✅ Dataset Processed! Shape:\", X_train.shape, y_train.shape)\n    return X_train, X_test, y_train, y_test, y_train_bin, y_test_bin\n\ndef train_models(dataset_name):\n    \"\"\"Trains and evaluates models on given dataset with selected metrics.\"\"\"\n    X_train, X_test, y_train, y_test, y_train_bin, y_test_bin = process_dataset(dataset_name)\n    \n    models = {\n        \"Linear Regression\": LinearRegression(),\n        \"SVR\": SVR(kernel=\"rbf\"),\n        \"Ridge Regression\": Ridge(alpha=1.0),\n        \"Random Forest\": RandomForestRegressor(n_estimators=100, random_state=42),\n        \"Gradient Boosting\": GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=42),\n        \"XGBoost\": XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42),\n        \"LightGBM\": LGBMRegressor(n_estimators=100, learning_rate=0.1, random_state=42),\n        \"MLP\": MLPRegressor(hidden_layer_sizes=(128, 64), max_iter=200, random_state=42)\n    }\n\n    results = []\n    for name, model in models.items():\n        print(f\"\\n🚀 Training {name} on {dataset_name}...\")\n        start_time = time.time()\n        model.fit(X_train, y_train)\n        y_pred = model.predict(X_test)\n        \n        # Metrics\n        mse = mean_squared_error(y_test, y_pred)\n        rmse = np.sqrt(mse)\n        mape = mean_absolute_percentage_error(y_test, y_pred) * 100\n        accuracy = 100 - mape\n        r2 = r2_score(y_test, y_pred)\n        pearson_r, _ = pearsonr(y_test, y_pred)\n\n        try:\n            y_prob = model.predict_proba(X_test)[:, 1] if hasattr(model, \"predict_proba\") else y_pred\n            auc_roc = roc_auc_score(y_test_bin, y_prob)\n        except:\n            auc_roc = None\n        \n        elapsed_time = time.time() - start_time\n        print(f\"📊 {name} Metrics:\")\n        print(f\"   - MSE: {mse:.4f}, RMSE: {rmse:.4f}, Accuracy: {accuracy:.2f}%, R²: {r2:.4f}, Pearson r: {pearson_r:.4f}, AUC-ROC: {auc_roc if auc_roc else 'N/A'}\")\n        \n        results.append({\n            \"Model\": name,\n            \"MSE\": mse,\n            \"RMSE\": rmse,\n            \"Accuracy (%)\": accuracy,\n            \"R2 Score\": r2,\n            \"Pearson r\": pearson_r,\n            \"AUC-ROC\": auc_roc if auc_roc is not None else \"N/A\"\n        })\n\n    df_results = pd.DataFrame(results)\n    filename = f\"{dataset_name.lower()}_classical_metrics.csv\"\n    df_results.to_csv(filename, index=False)\n    print(f\"\\n✅ Metrics saved to {filename}\")\n    return df_results\n\n# Run for all three datasets\nresults_kiba = train_models(\"KIBA\")\nresults_davis = train_models(\"Davis\")\nresults_bindingdb = train_models(\"BindingDB\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}